---
layout: post
current: post
cover:  assets/blogposts/014.png
navigation: True
title: Las P√°ginas Perdidas Parte 1
date: 2020-02-26 10:00:00
tags: [blog, python]
class: post-template
subclass: 'post tag-blog'
author: lewiserick
---
<h1>Las P√°ginas Perdidas Parte 1: Pruebas Estad√≠sticas de Bondad</h1>

Cuando observamos dos sets de datos, es muy tentador asumir que forman parte de diferentes grupos independientes i.e. no tienen relaciones entre ellos. Esta simplificaci√≥n nos permite evaluar las propiedades de cada grupo de manera aislada, pues similar a la probabilidad, los eventos independientes son m√°s f√°ciles de calcular. ¬øHabr√° veces que esto perjudique m√°s de lo que nos ayuda?

La respuesta es s√≠ pero no. Todo depende de la complejidad del fen√≥meno que estamos observando. Por ejemplo, es m√°s sencillo analizar los factores involucrados en un juego de gato que los factores que entran en juego en el tr√°fico de Monterrey. Como cient√≠ficos de datos, cada supuesto necesita evidencia y pruebas que lo respalden. En este art√≠culo pondremos nuestros supuestos a prueba mediante la prueba de <b>bondad de ajuste</b>.

<img src="assets/blogposts/014_imagen_1.jpg"><br>

Imagina que encontraste tres p√°ginas. No sabemos de qu√© libro vienen, o si vienen de libros diferentes. En realidad, dos vienen del mismo libro, de biolog√≠a y el tercero de un libro de cocina. Sin embargo, <b>no sabemos eso todav√≠a</b>. Vamos a buscar probarlo <b>ahora</b>.

Nuestro supuesto de las p√°ginas de los libros es una hip√≥tesis, la cual podemos aceptar a rechazar. ¬øQu√© informaci√≥n podr√≠amos utilizar para probar nuestra hip√≥tesis?

¬øPero qu√© es eso? ¬øAcaso un mago?
<img src="assets/blogposts/014_imagen_2.jpg"><br>

¬°Woosh! ¬°Woosh! ¬°Ahora tienes un tip!

El adorable mago nos da dos sospechosas pero convenientes pistas:
- N√∫mero de caracteres en cada palabra
- Palabras √∫nicas
En esta primera parte del <i>misterio</i>, analizaremos la primera pista.

¬øPero qu√© es una prueba de <b>bondad de ajuste</b>? Esta prueba mide cu√°nto una funci√≥n o muestra coincide con nuestras expectativas de c√≥mo se deber√≠a de ver, como otra funci√≥n o muestra. Probaremos la bondad de ajuste con la prueba Kolmogorov-Smirnov (KS para abreviar).

<b>¬øQu√© es la estad√≠stica Kolmogorov-Smirnov?</b>
La estad√≠stica KS mide la distance entre dos funciones cumulativas de densidad (FCD). Una FCD representa la probabilidad de obtener un valor de nuestra variable en particular si fueramos a elegir un valor de manera aleatoria. Lo que hace a la FCD especial es que cada cada valor de la variable es la suma de sus valores anteriores. Esta caracter√≠stica cumulativa permite FCD ser menos sensible a ruido en los datos. ¬øQu√© significa esto?

Echemos un vistazo a dos Funciones de Densidad de Probabilidad (FDP):
<img src="assets/blogposts/014_imagen_3.png"><br>

Las diferencias introducidas en las FDP de la Muestra 1 y Muestra 2 son:
- En la <b>FDP de la muestra 2</b>, faltan datos (representados por un 0) en algunos valores del rango de la variable ‚ÄúN√∫mero de caracteres en palabra‚Äù.
- Los valores no cero en la FDP de la muestra 1 comienzan con el valor de la variable zero, mientras que este cambio comienza tarde para la <b>FDP de la muestra 2</b>, comenzando con el valor de la variable dos.
- La ‚Äúcampana‚Äù de probabilidad de la muestra 1 termina en el valor 10, mientras que la muestra 2 parece continuar.

Si comparamos <b>punto por punto</b> la distancia entre cada punto en las dos FDP mostradas obtenemos una distancia bastante larga, especialmente para el valor de la cantidad de palabras con longitud de 10 caracteres. Con esta medida concluiriamos que las dos muestras no tiene una distribuci√≥n de probabilidad similar. Sin embargo, en aplicaciones pr√°cticas reales, nuestros datos pueden tener sesgo a trav√©s de:
- Informaci√≥n faltante para algunos valores de la variable, como los ceros en la <b>FDP de la muestra 2</b>
- Funciones de probabilidad desplazadas, o en otras palabras distribuciones con forma similar pero diferente centro, como podr√≠a interpretarse con las dos muestras.

Si utilizamos los datos de la FDP como est√°n sin hacerles alg√∫n preprocesamiento, hacemos nuestras conclusiones sensibles al ruido y al sesgo, lo cual permitir√≠a una gran variabilidad en nuestras mediciones. Hacer esto nos llevar√≠a a obtener un resultado diferente cada vez que tomamos nuevas muestras. Necesitamos una estrategia que sea capaz de ir m√°s all√° de esto. Necesitamos la Funci√≥n Cumulativa de Densidad (FCD):
<img src="assets/blogposts/014_imagen_4.png"><br>

La diferencia relativa entre las FCDs parece ser significativamente m√°s peque√±a que las de FDPs de las que originan. Si comparamos el Error Cuadr√°tico Medio (ECM) entre los FDP normalizados en escala (el valor m√°ximo es 1, en vez de menor a 1 para que tenga la misma escala que los FCDs) y los FCDs enconrramos que el ECM de los FDPs son 0.2, cinco veces mayor al valor de ECM de los FCDs con un valor de 0.04 (<a href="https://github.com/LewisErick/kolmogorov-smirnov/blob/master/Explaining%20CDFs.ipynb">notebook<a/>). <b>Excelente, ahora obtengamos las FCDs de las p√°ginas perdidas:</b>
<img src="assets/blogposts/014_imagen_5.png"><br>

<i>Bello, bello.</i> Pero, ¬øqu√© fue lo que hicimos?
- Convertir las palabras de las p√°ginas en n√∫meros que representan su cantidad de caracteres ‚úÖ
- Registrar cu√°ntas veces cierta longitud de palabra aparece en nuestras tres muestras ‚úÖ
- Dividir cada frecuencia obtenida del paso 2 por la cantidad de palabras totales. Esto nos dar√° la DFP para cada una de las muestras. ‚úÖ
- Para cada valor de longitud, sumar la probabilidad de los valores menores al actual. Esto nos dar√° la FCD emp√≠rica, o la Funci√≥n Emp√≠rica de Densidad (FED). ‚úÖ


Si nos quedamos con el ECM para determinar si las p√°ginas provienen del mismo libro, concluimos que eso es verdad. ¬øPero ser√° esto verdad?
¬°Aqu√≠ viene el mago con otra pista!
<img src="assets/blogposts/014_imagen_6.jpg"><br>

<b>No.</b> üê±

Por lo tanto, necesitamos encontrar una comparaci√≥n m√°s granular. La clave est√° en no relajar <b>tanto</b> nuestra comparaci√≥n, a√∫n habiendo transformado nuestras DFPs en FEDs. Si bien esta transformaci√≥n nos ayud√≥ a ser m√°s resilientes, <i>ser√≠a genial si pudi√©ramos utilizar algo diferente a ECM para nuestra comparaci√≥n que aproveche las propiedades de la FED.</i> Afortunadamente, la prueba KS nos tiene cubiertos! Disponible en numerosas librer√≠as de Python como Scipy, KS calcula dos valores, D y p, al comparar dos muestras:
<img src="assets/blogposts/014_imagen_7.png"><br>
En otras palabras, la diferencia m√°s grande entre los puntos de las funciones F y G.

El valor de D se utiliza para determinar el valor de p, resultado el cual es m√°s complicado de obtener (no en el alcance de este art√≠culo). Afortunadamente para nosotros, el valor de p ya nos lo dan nuestras <i>bellas librer√≠as</i>. Aqu√≠ est√°n los resultados de aplicar KS a nuestros tres fragmentos (<a href="https://github.com/LewisErick/kolmogorov-smirnov/blob/master/Kolmogorov-Smirnoff%20Test.ipynb">notebook</a>):
<img src="assets/blogposts/014_imagen_8.png"><br>

El valor de p puede interpretarse como una probabilidad del 0 al 1. <i>¬øPero probabilidad de qu√© exactamente?</i> ¬øDe que los fragmentos vienen de libros diferentes? <b>No necesariamente</b>. La √∫nica conclusi√≥n que nos da analizar la longitud de las palabras en los fragmentos es que es muy probable que el lenguaje entre los fragmentos sean muy similares entre las tres muestras.

Esto no es suficiente para determinar que provienen del mismo libro. Esto es un ejemplo cl√°sico de una caracter√≠stica que no nos permite capturar completamente la variabilidad del dominio del problema de d√≥nde viene.

<i>¬øQu√© haremos ahora?</i> La identidad de estas p√°ginas siempre estar√° cubierta de duda e incertidumbre?

¬°Aver√≠gualo en la siguiente edici√≥n!
